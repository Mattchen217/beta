from dataclasses import dataclass
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from dateutil.parser import isoparse
import re

from src.a_memory.config import MIN_TEXT_LEN, MAX_CHUNK_CHARS
from src.a_memory.preprocess import normalize_text

# â€œçº¯å¯’æš„/ç¡®è®¤â€å™ªå£°è¯ï¼šåªåœ¨ä¿¡æ¯é‡æä½æ—¶ä¸¢å¼ƒ
NOISE_WORDS = {"å—¯", "å¥½çš„", "å“ˆå“ˆ", "ok", "OK", "å¥½", "æ”¶åˆ°", "è¡Œ", "å¯ä»¥", "æ²¡é—®é¢˜", "thanks", "thx"}

# çŸ­æ–‡æœ¬ä½†ä¿¡æ¯å¯†åº¦é«˜ï¼šä¿ç•™ï¼ˆé‡‘é¢/æ•°å­—/æ—¥æœŸ/å…³é”®ç¬¦å·ï¼‰
INFO_DENSE_RE = (
    r"(\d{1,4}(\.\d+)?\s*(ä¸‡|w|k|K|å…ƒ|å—|ç¾å…ƒ|\$|Â¥))"  # é‡‘é¢/æ•°é‡
    r"|(\d{4}[-/]\d{1,2}[-/]\d{1,2})"                 # æ—¥æœŸ
    r"|(\d{1,2}:\d{2})"                               # æ—¶é—´
    r"|(#\w+|@\w+)"                                   # è¯é¢˜/æåŠ
)

@dataclass
class Chunk:
    chunk_id: str
    conv_id: str
    time_start: str
    time_end: str
    text: str
    message_ids: List[str]
    # å¯é€‰ï¼šç”¨äºæ›´å¼ºçš„å¯è§£é‡Šæ€§ï¼ˆbeta å¯ä»¥ä¸å…¥åº“ï¼‰
    senders: Optional[List[str]] = None

def _is_punct_only(t: str) -> bool:
    return all(ch in "ğŸ˜‚ğŸ¤£â€¦.,!?ï¼Œã€‚ï¼ï¼Ÿ" for ch in t)

def is_noise(text: str) -> bool:
    """
    å™ªå£°åˆ¤å®šåŸåˆ™ï¼š
    - å…è®¸çŸ­ï¼Œä½†ä¸èƒ½â€œæ— ä¿¡æ¯â€
    - å¦‚æœåŒ…å«æ•°å­—/é‡‘é¢/æ—¥æœŸç­‰ä¿¡æ¯ï¼Œå“ªæ€•å¾ˆçŸ­ä¹Ÿä¿ç•™
    """
    t = normalize_text(text)
    if not t:
        return True
    if _is_punct_only(t):
        return True
    if re.search(INFO_DENSE_RE, t):
        return False
    # ä½äºé˜ˆå€¼ä¸”å±äºå¯’æš„è¯ï¼Œä¸¢å¼ƒ
    if len(t) < MIN_TEXT_LEN and t in NOISE_WORDS:
        return True
    # ä½äºé˜ˆå€¼ä½†ä¸æ˜¯å¯’æš„è¯ï¼šä¿ç•™ï¼ˆä¾‹å¦‚â€œ20ä¸‡â€/â€œå‘äº†â€/â€œç­¾äº†â€ï¼‰
    if len(t) < MIN_TEXT_LEN:
        return False
    # é•¿æ–‡æœ¬ä½†åªæœ‰å¯’æš„
    if t in NOISE_WORDS:
        return True
    return False

import re

def build_chunks(
    conv_id: str,
    messages: List[Dict],
    *,
    max_messages: int = 8,
    min_messages: int = 2,
    time_gap_minutes: int = 30,
) -> List[Chunk]:
    """
    Beta ç‰ˆ chunkingï¼š
    - ä»¥â€œæ—¶é—´é—´éš” + æ¶ˆæ¯ä¸Šé™â€åˆ‡åˆ†ï¼ˆæ¯”å›ºå®š 3 æ¡ç¨³å®šå¾ˆå¤šï¼‰
    - ä¿è¯ chunk ä¸æˆªæ–­åœ¨æ¶ˆæ¯ä¸­é—´ï¼ˆä¸åšå­—ç¬¦ç¡¬æˆªæ–­ï¼›å¿…è¦æ—¶æ‹†æˆå¤šä¸ª chunkï¼‰
    - ä¿ç•™çŸ­ä½†ä¿¡æ¯å¯†åº¦é«˜çš„æ–‡æœ¬ï¼ˆä¿®å¤ MIN_TEXT_LEN è¯¯æ€ï¼‰
    """
    chunks: List[Chunk] = []
    buf = []  # (id, sender, ts, text)
    start_ts = None
    last_kept_ts = None

    def flush(end_ts: str):
        nonlocal buf, start_ts, last_kept_ts
        if not buf:
            return

        # ç»„è£…æ–‡æœ¬ï¼šå¸¦ sender æ ‡ç­¾ï¼Œä½†å°½é‡ç®€æ´
        lines = [f'{sender}: {txt}' for (_id, sender, _ts, txt) in buf]
        text_all = "\n".join(lines)

        # å¦‚è¶…è¿‡ MAX_CHUNK_CHARSï¼Œåˆ™æŒ‰æ¶ˆæ¯è¾¹ç•Œæ‹†åˆ†
        cur_lines, cur_ids, cur_senders = [], [], []
        cur_start = buf[0][2]
        cur_len = 0

        def emit(cur_end: str):
            if not cur_ids:
                return
            chunk_id = f"{conv_id}_{cur_ids[0]}_{cur_ids[-1]}"
            chunks.append(Chunk(
                chunk_id=chunk_id,
                conv_id=conv_id,
                time_start=cur_start,
                time_end=cur_end,
                text="\n".join(cur_lines),
                message_ids=list(cur_ids),
                senders=list(cur_senders),
            ))

        for (_id, sender, ts, txt) in buf:
            line = f"{sender}: {txt}"
            # +1 æ˜¯æ¢è¡Œ
            added = len(line) + (1 if cur_lines else 0)
            if cur_lines and (cur_len + added) > MAX_CHUNK_CHARS:
                emit(prev_ts)
                # reset
                cur_lines, cur_ids, cur_senders = [], [], []
                cur_start = ts
                cur_len = 0

            cur_lines.append(line)
            cur_ids.append(_id)
            cur_senders.append(sender)
            cur_len += added
            prev_ts = ts

        # emit tail
        emit(buf[-1][2])

        # reset buffer
        buf = []
        start_ts = None
        last_kept_ts = None

    gap = timedelta(minutes=time_gap_minutes)

    for m in messages:
        raw = m.get("text", "")
        txt = normalize_text(raw)
        if is_noise(txt):
            continue

        ts = m["ts"]
        if start_ts is None:
            start_ts = ts

        # æ—¶é—´é—´éš”åˆ‡åˆ†ï¼ˆä¸ä¸Šä¸€æ¡ä¿ç•™æ¶ˆæ¯æ¯”è¾ƒï¼‰
        if last_kept_ts is not None:
            if isoparse(ts) - isoparse(last_kept_ts) > gap and len(buf) >= 1:
                flush(last_kept_ts)

        buf.append((m["id"], m["sender"], ts, txt))
        last_kept_ts = ts

        # æ¶ˆæ¯ä¸Šé™åˆ‡åˆ†
        if len(buf) >= max_messages:
            flush(ts)

    # æ”¶å°¾ï¼šè‡³å°‘ min_messages æ‰å•ç‹¬æˆå—ï¼Œå¦åˆ™å¹¶å…¥ä¸Šä¸€å—ï¼ˆbeta ç®€åŒ–ï¼šç›´æ¥ flushï¼‰
    if buf:
        flush(last_kept_ts or messages[-1]["ts"])

    # åˆå¹¶è¿‡ç¢çš„ chunkï¼ˆ<min_messagesï¼‰åˆ°å‰ä¸€ä¸ªï¼šé¿å…ç¢ç‰‡å½±å“æ£€ç´¢
    merged: List[Chunk] = []
    for c in chunks:
        if merged and len(c.message_ids) < min_messages:
            prev = merged[-1]
            prev.text = prev.text + "\n" + c.text
            prev.time_end = c.time_end
            prev.message_ids.extend(c.message_ids)
            if prev.senders and c.senders:
                prev.senders.extend(c.senders)
        else:
            merged.append(c)

    return merged
